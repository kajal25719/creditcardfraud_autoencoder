{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"creditcardfraud.ipynb","private_outputs":true,"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"RtD9iFSLu_tA"},"source":["## This dataset comes from Kaggle - https://www.kaggle.com/dalpozz/creditcardfraud\n","## This tutorial demonstrates how to use unsupervised training for fraud detection. The main model implemented here is auto-encoder, which achives 0.95 of AUC on test set. \n","### Author: Weimin Wang\n","### Date: June 18, 2017"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Cvx-sY0Iu_tL"},"source":["## Load libraries "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"JeYKxGyYu_tP"},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from datetime import datetime \n","from sklearn.metrics import roc_auc_score as auc \n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"incU-nr_u_tS"},"source":["import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rkIJP-_Pu_tT"},"source":["## Data Exploration"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"RyJ3WH2Eu_tU"},"source":["df = pd.read_csv('/creditcard.csv.zip')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fh0Pkl8Qu_tV"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYFHksnHu_tY"},"source":["print(\"Total time spanning: {:.1f} days\".format(df['Time'].max() / (3600 * 24.0)))\n","print(\"{:.3f} % of all transactions are fraud. \".format(np.sum(df['Class']) / df.shape[0] * 100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuC-Pmewu_ta"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARH5_LdLu_tc"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6b-6Ap8u_te"},"source":["df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AiFnDkGwu_te"},"source":["plt.figure(figsize=(12,5*4))\n","gs = gridspec.GridSpec(5, 1)\n","for i, cn in enumerate(df.columns[:5]):\n","    ax = plt.subplot(gs[i])\n","    sns.distplot(df[cn][df.Class == 1], bins=50)\n","    sns.distplot(df[cn][df.Class == 0], bins=50)\n","    ax.set_xlabel('')\n","    ax.set_title('histogram of feature: ' + str(cn))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NwGN3Y7u_tg"},"source":["## Train and test split on time series, using first 75% as training/val, and last 25% as test"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"c_dpMR6ou_th"},"source":["TEST_RATIO = 0.25\n","df.sort_values('Time', inplace = True)\n","TRA_INDEX = int((1-TEST_RATIO) * df.shape[0])\n","train_x = df.iloc[:TRA_INDEX, 1:-2].values\n","train_y = df.iloc[:TRA_INDEX, -1].values\n","\n","test_x = df.iloc[TRA_INDEX:, 1:-2].values\n","test_y = df.iloc[TRA_INDEX:, -1].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7CUj7L_u_ti"},"source":["print(\"Total train examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(train_x.shape[0], np.sum(train_y), np.sum(train_y)/train_x.shape[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnmm_Flru_tj"},"source":["print(\"Total test examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(test_x.shape[0], np.sum(test_y), np.sum(test_y)/test_y.shape[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iccjXJdKu_tk"},"source":["## Feature Normalization - min max score (used for sigmoid activation)"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"LIBajzcIu_tk"},"source":["'''cols_max = []\n","cols_min = []\n","for c in range(train_x.shape[1]):\n","    cols_max.append(train_x[:,c].max())\n","    cols_min.append(train_x[:,c].min())\n","    train_x[:, c] = (train_x[:, c] - cols_min[-1]) / (cols_max[-1] - cols_min[-1])\n","    test_x[:, c] =  (test_x[:, c] - cols_min[-1]) / (cols_max[-1] - cols_min[-1])'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFcMyO2ru_tl"},"source":["## Feature Normalization 2 - z score (for tanh activation)"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"juxrqoJZu_tl"},"source":["cols_mean = []\n","cols_std = []\n","for c in range(train_x.shape[1]):\n","    cols_mean.append(train_x[:,c].mean())\n","    cols_std.append(train_x[:,c].std())\n","    train_x[:, c] = (train_x[:, c] - cols_mean[-1]) / cols_std[-1]\n","    test_x[:, c] =  (test_x[:, c] - cols_mean[-1]) / cols_std[-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"avz9vCuHu_tm"},"source":["## Modelling and results\n"]},{"cell_type":"markdown","metadata":{"id":"OE08XdEZu_tn"},"source":["## 1. Auto-encoder as unsupervised learning"]},{"cell_type":"markdown","metadata":{"id":"zrniBAANu_tn"},"source":["### Parameters"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"t_xOB3nEu_to"},"source":["# Parameters\n","learning_rate = 0.001\n","training_epochs = 10\n","batch_size = 256\n","display_step = 1\n","\n","# Network Parameters\n","n_hidden_1 = 15 # 1st layer num features\n","#n_hidden_2 = 15 # 2nd layer num features\n","n_input = train_x.shape[1] # MNIST data input (img shape: 28*28)\n","data_dir = '.'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pl6vy8i0u_to"},"source":["### Train and val the model - (1 hidden layer turned out to be enough)"]},{"cell_type":"code","metadata":{"id":"YLjUccr1u_tp"},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","X = tf.placeholder(\"float\", [None, n_input])\n","\n","\n","weights = {\n","    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n","    #'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n","    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n","    #'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n","}\n","biases = {\n","    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n","    #'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n","    'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n","    #'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n","}\n","\n","\n","# Building the encoder\n","def encoder(x):\n","    # Encoder Hidden layer with sigmoid activation #1\n","    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']),\n","                                   biases['encoder_b1']))\n","    # Decoder Hidden layer with sigmoid activation #2\n","    #layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n","                                   #biases['encoder_b2']))\n","    return layer_1\n","\n","\n","# Building the decoder\n","def decoder(x):\n","    # Encoder Hidden layer with sigmoid activation #1\n","    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h1']),\n","                                   biases['decoder_b1']))\n","    # Decoder Hidden layer with sigmoid activation #2\n","    #layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n","                                  # biases['decoder_b2']))\n","    return layer_1\n","\n","# Construct model\n","encoder_op = encoder(X)\n","decoder_op = decoder(encoder_op)\n","\n","# Prediction\n","y_pred = decoder_op\n","# Targets (Labels) are the input data.\n","y_true = X\n","\n","# Define batch mse\n","batch_mse = tf.reduce_mean(tf.pow(y_true - y_pred, 2), 1)\n","\n","# Define loss and optimizer, minimize the squared error\n","cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n","optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n","\n","# TRAIN StARTS\n","save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n","saver = tf.train.Saver()\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    now = datetime.now()\n","    sess.run(init)\n","    total_batch = int(train_x.shape[0]/batch_size)\n","    # Training cycle\n","    for epoch in range(training_epochs):\n","        # Loop over all batches\n","        for i in range(total_batch):\n","            batch_idx = np.random.choice(train_x.shape[0], batch_size)\n","            batch_xs = train_x[batch_idx]\n","            # Run optimization op (backprop) and cost op (to get loss value)\n","            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n","            \n","        # Display logs per epoch step\n","        if epoch % display_step == 0:\n","            train_batch_mse = sess.run(batch_mse, feed_dict={X: train_x})\n","            print(\"Epoch:\", '%04d' % (epoch+1),\n","                  \"cost=\", \"{:.9f}\".format(c), \n","                  \"Train auc=\", \"{:.6f}\".format(auc(train_y, train_batch_mse)), \n","                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n","\n","    print(\"Optimization Finished!\")\n","    \n","    save_path = saver.save(sess, save_model)\n","    print(\"Model saved in file: %s\" % save_path)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"QA3zX0UZu_tq"},"source":["### Test model - on later 25% test data"]},{"cell_type":"code","metadata":{"id":"5hH730ZAu_tr"},"source":["save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n","saver = tf.train.Saver()\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    now = datetime.now()\n","    \n","    saver.restore(sess, save_model)\n","    \n","    test_batch_mse = sess.run(batch_mse, feed_dict={X: test_x})\n","    \n","    print(\"Test auc score: {:.6f}\".format(auc(test_y, test_batch_mse)))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"99yk4DOqu_tr"},"source":["## Visualize the prediction"]},{"cell_type":"markdown","metadata":{"id":"7GB4anQ-u_ts"},"source":["### 1. Display fraud score (mse) distribution for non-fraud cases"]},{"cell_type":"code","metadata":{"id":"wqRZvBTvu_ts"},"source":["plt.hist(test_batch_mse[test_y == 0.0], bins = 100)\n","plt.title(\"fraud score (mse) distribution of non-fraud cases.\")\n","plt.xlabel(\"fraud score (mse)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vboO_CQXu_tu"},"source":["### Zoom into (0, 20) range"]},{"cell_type":"code","metadata":{"id":"ZMX9fheZu_tu"},"source":["plt.hist(test_batch_mse[(test_y == 0.0) & (test_batch_mse < 20)], bins = 100)\n","plt.title(\"fraud score (mse) distribution for non-fraud cases.\")\n","plt.xlabel(\"fraud score (mse)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yjmh76a6u_tv"},"source":["### 2. Display fraud score (mse) distribution for fraud cases"]},{"cell_type":"code","metadata":{"id":"p-M0YFAku_tv"},"source":["plt.hist(test_batch_mse[test_y == 1.0], bins = 100)\n","plt.title(\"fraud score (mse) distribution for fraud cases.\")\n","plt.xlabel(\"fraud score (mse)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ECqCsv6zu_tw"},"source":["THRE_TEST = 7\n","print(\"Let's, for example, use 7 as our detection threshold: \\n\\\n","Number of detected cases above treshold: {}, \\n\\\n","Number of pos cases only above threshold: {}, \\n\\\n","The percentage of accuracy above treshold (Precision): {:0.2f}%. \\n\\\n","Compared to the average percentage of fraud in test set: 0.132%\".format( \\\n","np.sum(test_batch_mse > THRE_TEST), \\\n","np.sum(test_y[test_batch_mse > THRE_TEST]), \\\n","np.sum(test_y[test_batch_mse > THRE_TEST]) / np.sum(test_batch_mse > THRE_TEST) * 100))\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6r7BOCbLu_tw"},"source":["### Observation: Our precision increased by a factor of 60 from 0.132% to 7.86%; However, the detection precision is still low (below 8%), but this is mainly due to the overall percentage of fraud cases is really too low. "]},{"cell_type":"markdown","metadata":{"id":"pWhFyVkXu_tx"},"source":["## 2. Build a binary classifier that predicts fraud, using the auto-encoder embedding layers as model inputs. "]},{"cell_type":"markdown","metadata":{"id":"C-2jHpr-u_tx"},"source":["### 1) Get auto-encoder embedding (the `encoder_op` tensor) for both train and test data"]},{"cell_type":"code","metadata":{"id":"5W1fxvWuu_ty"},"source":["save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\n","saver = tf.train.Saver()\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    now = datetime.now()\n","    saver.restore(sess, save_model)\n","    \n","    test_encoding = sess.run(encoder_op, feed_dict={X: test_x})\n","    train_encoding = sess.run(encoder_op, feed_dict={X: train_x})\n","    \n","    print(\"Dim for test_encoding and train_encoding are: \\n\", test_encoding.shape, '\\n', train_encoding.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iEnd5UPTu_ty"},"source":["### 2) Build the graph for FC layers (best hidden size based on validation is found to be 4)"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"xYOYS036u_ty"},"source":["#n_input = test_encoding.shape[1]\n","n_input = test_encoding.shape[1]\n","\n","hidden_size = 4\n","output_size = 2\n","\n","X = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n","y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n","\n","weights = {\n","    'W1': tf.Variable(tf.truncated_normal([n_input, hidden_size])),\n","    'W2': tf.Variable(tf.truncated_normal([hidden_size, output_size])),\n","}\n","biases = {\n","    'b1': tf.Variable(tf.zeros([hidden_size])),\n","    'b2': tf.Variable(tf.zeros([output_size])),\n","}\n","\n","hidden_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n","pred_logits = tf.add(tf.matmul(hidden_layer, weights['W2']), biases['b2'])\n","pred_probs = tf.nn.softmax(pred_logits)\n","\n","cross_entropy = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n","\n","optimizer = tf.train.AdamOptimizer(2e-4).minimize(cross_entropy)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTsUU3UYu_tz"},"source":["### 3) Prepare the data set. \n","### Now we need to re-split the train into train/val, due to supervised training.\n","### We will therefore use 80% of out previous training data as our new training, and the remaining 20% as new validation. (train : val : test = (0.75x0.8) : (0.75x0.2) : (0.25x1.0)). \n","### Finally we start to train our binary classifier"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"faEhc633u_tz"},"source":["n_epochs = 80\n","batch_size = 256\n","\n","# PREPARE DATA\n","VAL_PERC = 0.2\n","all_y_bin = np.zeros((df.shape[0], 2))\n","all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n","\n","train_enc_x = train_encoding[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n","train_enc_y = all_y_bin[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n","\n","val_enc_x = train_encoding[int(train_encoding.shape[0] * (1-VAL_PERC)):]\n","val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_encoding.shape[0]]\n","\n","test_enc_y = all_y_bin[train_encoding.shape[0]:]\n","print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n","                                                                        test_encoding.shape[0]))\n","\n","# TRAIN STARTS\n","save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n","saver = tf.train.Saver()\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    now = datetime.now()\n","    sess.run(init)\n","    total_batch = int(train_enc_x.shape[0]/batch_size)\n","    # Training cycle\n","    for epoch in range(n_epochs):\n","        # Loop over all batches\n","        for i in range(total_batch):\n","            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n","            batch_xs = train_enc_x[batch_idx]\n","            batch_ys = train_enc_y[batch_idx]\n","\n","            # Run optimization op (backprop) and cost op (to get loss value)\n","            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n","            \n","        # Display logs per epoch step\n","        if epoch % display_step == 0:\n","            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n","            print(\"Epoch:\", '%04d' % (epoch+1),\n","                  \"cost=\", \"{:.9f}\".format(c), \n","                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n","                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n","\n","    print(\"Optimization Finished!\")\n","    \n","    save_path = saver.save(sess, save_model)\n","    print(\"Model saved in file: %s\" % save_path)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Qnm95lau_t1"},"source":["### 4) Test the model on the same test data as before - improved on AUC slightly - 0.9669"]},{"cell_type":"code","metadata":{"id":"O21kCAHFu_t2"},"source":["save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\n","saver = tf.train.Saver()\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    now = datetime.now()\n","    \n","    saver.restore(sess, save_model)\n","    \n","    test_probs = sess.run(pred_probs, feed_dict={X: test_encoding})\n","    \n","    print(\"\\nTest auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gUMHT6Epu_t2"},"source":["## 3. (Optional) However, let's test a simple supervisied neural network (two layers FC) from scratch - without using auto-encoder"]},{"cell_type":"markdown","metadata":{"id":"2uH9Sentu_t3"},"source":["### 1) Build graph - 28 (input) -> 8 -> 4 -> 2"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"TdJxIH83u_t3"},"source":["n_epochs = 200\n","batch_size = 256\n","\n","#n_input = test_encoding.shape[1]\n","n_input = train_x.shape[1]\n","\n","hidden1_size = 8\n","hidden2_size = 4\n","output_size = 2\n","\n","X = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n","y_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n","\n","weights = {\n","    'W1': tf.Variable(tf.truncated_normal([n_input, hidden1_size])),\n","    'W2': tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size])),\n","    'W3': tf.Variable(tf.truncated_normal([hidden2_size, output_size])),\n","}\n","biases = {\n","    'b1': tf.Variable(tf.zeros([hidden1_size])),\n","    'b2': tf.Variable(tf.zeros([hidden2_size])),\n","    'b3': tf.Variable(tf.zeros([output_size])),\n","}\n","\n","hidden1_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\n","hidden2_layer =  tf.nn.relu(tf.add(tf.matmul(hidden1_layer, weights['W2']), biases['b2']))\n","pred_logits = tf.add(tf.matmul(hidden2_layer, weights['W3']), biases['b3'])\n","pred_probs = tf.nn.softmax(pred_logits)\n","\n","cross_entropy = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n","\n","optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_yQwquAu_t3"},"source":["### 2) Prepare the data set. Now we need to re-split the train into train/val. \n","### Again, we will use 80% of out previous training data as our new training, and the remaining 20% as new validation. (train : val : test = (0.75x0.8) : (0.75x0.2) : (0.25x1.0)).\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"bf8iX8uiu_t4"},"source":["# PREPARE DATA\n","VAL_PERC = 0.2\n","all_y_bin = np.zeros((df.shape[0], 2))\n","all_y_bin[range(df.shape[0]), df['Class'].values] = 1\n","\n","train_enc_x = train_x[:int(train_x.shape[0] * (1-VAL_PERC))]\n","train_enc_y = all_y_bin[:int(train_x.shape[0] * (1-VAL_PERC))]\n","\n","val_enc_x = train_x[int(train_encoding.shape[0] *  (1-VAL_PERC)):]\n","val_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_x.shape[0]]\n","\n","test_enc_y = all_y_bin[train_x.shape[0]:]\n","\n","print(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n","                                                                        test_encoding.shape[0]))\n","# TRAIN STARTS\n","save_model = os.path.join(data_dir, 'temp_saved_model_FCNNets_raw.ckpt')\n","saver = tf.train.Saver()\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    now = datetime.now()\n","    sess.run(init)\n","    total_batch = int(train_enc_x.shape[0]/batch_size)\n","    # Training cycle\n","    for epoch in range(n_epochs):\n","        # Loop over all batches\n","        for i in range(total_batch):\n","            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n","            batch_xs = train_enc_x[batch_idx]\n","            batch_ys = train_enc_y[batch_idx]\n","            # Run optimization op (backprop) and cost op (to get loss value)\n","            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n","            \n","        # Display logs per epoch step\n","        if epoch % display_step == 0:\n","            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n","            print(\"Epoch:\", '%04d' % (epoch+1),\n","                  \"cost=\", \"{:.9f}\".format(c), \n","                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n","                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n","\n","    print(\"Optimization Finished!\")\n","    \n","    save_path = saver.save(sess, save_model)\n","    print(\"Model saved in file: %s\" % save_path)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GScm0rBeu_t5"},"source":["### 3) Predict on test data"]},{"cell_type":"code","metadata":{"id":"qFy6xZmHu_t5"},"source":["save_model = os.path.join(data_dir, 'temp_saved_model_FCNNets_raw.ckpt')\n","saver = tf.train.Saver()\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    now = datetime.now()\n","    \n","    saver.restore(sess, save_model)\n","    \n","    test_probs = sess.run(pred_probs, feed_dict={X: test_x})\n","    \n","    print(\"Test auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"x--17n-iu_t6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"XpHw5upfu_t6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"O0YrKoPfu_t6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"yc34XXMNu_t6"},"source":[""],"execution_count":null,"outputs":[]}]}